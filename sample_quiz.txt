[ 
    {
        "question": "Which of the following best describes the idea behind self-attention mechanism?",
        "options": [
            "Each token attends to all other tokens in the sequence to compute a contextual representation.",
            "A mechanism where the model uses hard-coded attention to predefined keywords.",
            "A technique where only the first and last tokens of a sequence influence the output.",
            "A filtering process that removes redundant tokens before prediction."
        ],
        "answer": "Each token attends to all other tokens in the sequence to compute a contextual representation."
    },
    {
        "question": "What is the main goal of using cross-attention mechanism in large language models?",
        "options": [
            "To allow the decoder to focus on relevant parts of the encoder’s outputs.",
            "To improve memory usage by pruning unimportant tokens.",
            "To make tokens within a single sequence attend only to themselves.",
            "To merge multiple token sequences into one unified vector."
        ],
        "answer": "To allow the decoder to focus on relevant parts of the encoder’s outputs."
    },
    {
        "question": "What is the role of Q (query), K (key), and V (value) vectors in the self-attention mechanism?",
        "options": [
            "They define how each token attends to others by computing attention weights and combining values.",
            "They are static embeddings assigned before training.",
            "They store gradients for updating model parameters.",
            "They represent token frequency statistics in the training data."
        ],
        "answer": "They define how each token attends to others by computing attention weights and combining values."
    },
    {
        "question": "How are input embeddings different from attention output vectors in a transformer model?",
        "options": [
            "Input embeddings are static representations of tokens, while attention outputs are dynamic and context-aware.",
            "Attention vectors are used only during training, not inference.",
            "Input embeddings are derived from model weights, while attention outputs come from user input.",
            "They are the same, just reshaped for computation."
        ],
        "answer": "Input embeddings are static representations of tokens, while attention outputs are dynamic and context-aware."
    },
    {
        "question": "How is computation parallelized during training and inference in transformer models?",
        "options": [
            "Training is parallelized over tokens and attention heads, while inference uses KV caching for token-wise generation.",
            "Both training and inference operate strictly in a single-threaded fashion.",
            "Inference parallelizes by batching multiple models at once.",
            "Training is done sequentially to preserve token order."
        ],
        "answer": "Training is parallelized over tokens and attention heads, while inference uses KV caching for token-wise generation."
    },
    {
        "question": "What is the main purpose of Layer Normalization in transformers?",
        "options": [
            "To stabilize training by keeping activations within a consistent scale.",
            "To reduce the number of tokens by eliminating outliers.",
            "To reorder sequence elements before attention.",
            "To quantize weights for compression."
        ],
        "answer": "To stabilize training by keeping activations within a consistent scale."
    },
    {
        "question": "Why are residual connections used in transformer architectures?",
        "options": [
            "To preserve input information and allow gradients to flow more easily during backpropagation.",
            "To skip training for certain layers entirely.",
            "To normalize the output sequence lengths.",
            "To mask out padding tokens automatically."
        ],
        "answer": "To preserve input information and allow gradients to flow more easily during backpropagation."
    },
    {
        "question": "What does the temperature parameter control in the softmax function during text generation?",
        "options": [
            "It adjusts the randomness of token selection by scaling logits before softmax.",
            "It controls the learning rate of the optimizer.",
            "It sets the number of attention heads.",
            "It regulates how many layers to use in inference."
        ],
        "answer": "It adjusts the randomness of token selection by scaling logits before softmax."
    },
    {
        "question": "What happens if the temperature in softmax is set to zero during generation?",
        "options": [
            "The model picks the highest-probability token deterministically (argmax).",
            "It produces a runtime division-by-zero error.",
            "It results in fully random token sampling.",
            "It disables the model’s ability to predict next tokens."
        ],
        "answer": "The model picks the highest-probability token deterministically (argmax)."
    },
    {
        "question": "What does the perplexity metric evaluate in language models?",
        "options": [
            "How well a model predicts the next token, with lower values indicating better performance.",
            "The diversity of token outputs in sampling.",
            "The runtime speed of inference.",
            "The number of tokens the model can attend to at once."
        ],
        "answer": "How well a model predicts the next token, with lower values indicating better performance."
    },
    {
        "question": "Why is perplexity a differentiable metric during training?",
        "options": [
            "It is derived from the log-likelihood loss, which is differentiable with respect to model parameters.",
            "It is computed from gradients during backpropagation.",
            "It uses hard argmax operations that are differentiable.",
            "It depends on token frequency alone and not model weights."
        ],
        "answer": "It is derived from the log-likelihood loss, which is differentiable with respect to model parameters."
    },
    {
        "question": "What do scaling laws describe in the context of large language models?",
        "options": [
            "How performance improves predictably with increases in data, model size, and compute.",
            "How weights are scaled down during quantization.",
            "How models reduce floating point precision dynamically.",
            "How output sequences are padded to uniform length."
        ],
        "answer": "How performance improves predictably with increases in data, model size, and compute."
    },
    {
        "question": "How does parameter quantization help with deploying large language models efficiently?",
        "options": [
            "It reduces the precision of weights to save memory and speed up inference.",
            "It increases the precision of weights to improve output accuracy.",
            "It compresses token embeddings into binary for lossless storage.",
            "It shifts weights into frequency space for easier analysis."
        ],
        "answer": "It reduces the precision of weights to save memory and speed up inference."
    },
    {
        "question": "Why is the KV cache important for efficient transformer inference?",
        "options": [
            "It stores key and value tensors from previous tokens to avoid recomputation.",
            "It saves training gradients to reuse across epochs.",
            "It accumulates optimizer states for adaptive learning.",
            "It caches entire input sequences to reduce memory use."
        ],
        "answer": "It stores key and value tensors from previous tokens to avoid recomputation."
    },
    {
        "question": "Where in a Transformer architecture are Mixture of Experts (MoE) layers most commonly applied?",
        "options": [
            "In the attention heads to allow experts to specialize on token relationships.",
            "In the input embedding layer to encode domain-specific vocabularies.",
            "In the feed-forward network layers to selectively activate different expert MLPs.",
            "In the output layer to rank expert completions."
        ],
        "answer": "In the feed-forward network layers to selectively activate different expert MLPs."
    },
    {
        "question": "What is a key benefit of using Mixture of Experts in Transformer models?",
        "options": [
            "It makes models smaller by pruning attention heads.",
            "It increases performance by reusing positional encodings.",
            "It allows models to scale with more parameters while keeping inference compute constant.",
            "It replaces tokenization with expert-based segmentation."
        ],
        "answer": "It allows models to scale with more parameters while keeping inference compute constant."
    },
    {
        "question": "Why do large language models typically train for only one epoch over massive datasets?",
        "options": [
            "Because the dataset is extremely clean and well-curated.",
            "Training for more than one epoch usually leads to catastrophic forgetting.",
            "The dataset is so large that just one pass is sufficient to generalize well.",
            "Because GPUs can only handle one epoch before memory resets."
        ],
        "answer": "The dataset is so large that just one pass is sufficient to generalize well."
    },
    {
        "question": "What does rejection sampling aim to achieve in the context of LLM inference?",
        "options": [
            "It removes harmful tokens from training data.",
            "It scores multiple candidate outputs and selects the best according to a reward model.",
            "It samples only from high-frequency tokens to reduce noise.",
            "It penalizes repeated n-grams during generation."
        ],
        "answer": "It scores multiple candidate outputs and selects the best according to a reward model."
    },
    {
        "question": "What is the purpose of rejection sampling in LLM alignment?",
        "options": [
            "To prune out low-likelihood tokens during beam search.",
            "To train the model on only the rejected completions.",
            "To select the best response from multiple generated outputs using a reward model.",
            "To inject randomness into training for better generalization."
        ],
        "answer": "To select the best response from multiple generated outputs using a reward model."
    },
    {
        "question": "During RLHF, when can rejection sampling outputs be used for training?",
        "options": [
            "Only during supervised fine-tuning.",
            "Never — rejection sampling is only for inference.",
            "To construct a preference dataset or train a reward model for PPO.",
            "Only after converting them into embeddings."
        ],
        "answer": "To construct a preference dataset or train a reward model for PPO."
    },
    {
        "question": "In DPO (Direct Preference Optimization), why is the model trained on the difference between log-probs of chosen vs rejected completions?",
        "options": [
            "Because the rejected completion provides a stronger gradient signal.",
            "Because it allows the model to minimize variance in the output length.",
            "Because human feedback only gives us relative preferences, not ground-truth answers.",
            "Because it simulates a binary classification task between correct and incorrect tokens."
        ],
        "answer": "Because human feedback only gives us relative preferences, not ground-truth answers."
    },
    {
        "question": "How does PPO give fine-grained control over a language model's behavior?",
        "options": [
            "By directly maximizing the log-probability of preferred responses.",
            "By using a reward model that can encode multiple objectives like safety, helpfulness, and conciseness.",
            "By randomly sampling experts and penalizing long outputs.",
            "By freezing the attention weights and only tuning the output head."
        ],
        "answer": "By using a reward model that can encode multiple objectives like safety, helpfulness, and conciseness."
    },
    {
        "question": "What makes DPO more sample-efficient and simpler than PPO?",
        "options": [
            "It only requires unlabeled text data and no model training.",
            "It doesn’t need a reward model or reinforcement learning infrastructure.",
            "It uses precomputed reward values to fine-tune logits.",
            "It can be applied before the supervised fine-tuning stage."
        ],
        "answer": "It doesn’t need a reward model or reinforcement learning infrastructure."
    },
    {
        "question": "How can you simulate multi-objective behavior in DPO if you can't use a reward function?",
        "options": [
            "By encoding objectives as special tokens in the prompt.",
            "By instructing annotators to consistently favor one type of behavior over another when ranking responses.",
            "By applying temperature scaling to all rejected responses.",
            "By generating responses from multiple models and averaging their logits."
        ],
        "answer": "By instructing annotators to consistently favor one type of behavior over another when ranking responses."
    },
    {
        "question": "Which of the following best summarizes the difference between PPO and DPO?",
        "options": [
            "PPO requires human preferences; DPO doesn't.",
            "PPO is used only at inference time; DPO is used for training.",
            "PPO learns from reward scores; DPO learns directly from pairwise preferences.",
            "PPO works only for small models; DPO is for large models only."
        ],
        "answer": "PPO learns from reward scores; DPO learns directly from pairwise preferences."
    },
    {
        "question": "What is the main idea behind Direct Preference Optimization (DPO)?",
        "options": [
            "Using reinforcement learning to maximize rewards from a preference model.",
            "Using a reward model to choose between completions.",
            "Using a logistic loss to directly increase the probability of preferred completions over rejected ones.",
            "Using contrastive learning to minimize token-level entropy."
        ],
        "answer": "Using a logistic loss to directly increase the probability of preferred completions over rejected ones."
    },
    {
        "question": "What is a limitation of DPO compared to PPO?",
        "options": [
            "DPO cannot scale to large datasets.",
            "DPO can’t incorporate external signals like code correctness or response latency.",
            "DPO requires complex distributed infrastructure.",
            "DPO fine-tunes embeddings only, not full model weights."
        ],
        "answer": "DPO can’t incorporate external signals like code correctness or response latency."
    }
]